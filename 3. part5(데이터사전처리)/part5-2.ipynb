{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992eea79-8676-4a56-9e24-e35cff7c62d5",
   "metadata": {},
   "source": [
    "### 누락데이터 치환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09a4bc-1ba6-458a-9593-c942b4894b7b",
   "metadata": {},
   "source": [
    "- 데이터셋의 품질을 높일 목적으로 누락데이터를 무작정 삭제해 버린다면 어렵게 수집한 데이터를 활용하지 못하게 된다.\n",
    "- 데이터 분석의 정확도는 데이터의품질 외에도 제공된는 데이터의 양에 의해 상당한 영향을 받는다. 따라서 데이터 중에서 일부가 누락되어 있더라도 나머지 데이터를 최대한 살려서 데이터 분석에 활용하는 것이 좋은 결과를 얻는 경우가 많다.\n",
    "- 누락 데이터를 바꿔서 대체할 값으로는 데이터 분포와 특성을 잘 나타낼 수 있는 평균값, 최빈값 들이 있다. \n",
    "- 판다스에서 fillna() 메소드로 편리하게 처리를 한다. 이 메소드는 새로운 객체를 반환하기 때문에 원본 객체를 변경하려면 inplace=True옵션을 추가해야함.\n",
    "- 평균값 혹은 중간값을 변수로 담아서 fillna() 메소드에 인자로 전달하면 NaN값이 치환이 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b759784-54e1-4475-9a73-5eea69bda50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3215fddb-df0b-489c-86c9-118283101ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8eda0f2-b7a2-45ba-98f9-1d2880105dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22.0\n",
      "1    38.0\n",
      "2    26.0\n",
      "3    35.0\n",
      "4    35.0\n",
      "5     NaN\n",
      "6    54.0\n",
      "7     2.0\n",
      "8    27.0\n",
      "9    14.0\n",
      "Name: age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['age'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8ef579-01eb-4c39-b3db-fa93326ab188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22.000000\n",
      "1    38.000000\n",
      "2    26.000000\n",
      "3    35.000000\n",
      "4    35.000000\n",
      "5    29.699118\n",
      "6    54.000000\n",
      "7     2.000000\n",
      "8    27.000000\n",
      "9    14.000000\n",
      "Name: age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#평균값으로 값 대치하기\n",
    "mean_age = df['age'].mean(axis=0)\n",
    "df['age'].fillna(mean_age, inplace=True)\n",
    "\n",
    "print(df['age'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "789402f4-b03e-41a6-801f-0b71c5daa459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22.000000\n",
      "1    38.000000\n",
      "2    26.000000\n",
      "3    35.000000\n",
      "4    35.000000\n",
      "5    29.699118\n",
      "6    54.000000\n",
      "7     2.000000\n",
      "8    27.000000\n",
      "9    14.000000\n",
      "Name: age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 중간값으로 NaN값 대치하기\n",
    "median_age = df['age'].median(axis=0)\n",
    "df['age'].fillna(median_age, inplace=True)\n",
    "\n",
    "print(df['age'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a816a-11b4-42c9-a346-a21304eb4033",
   "metadata": {},
   "source": [
    "수치형 변수는 이런 값으로 대체할 수 있지만 범주형 변수에 대한 값을 최빈값으로 대치하고 싶다면??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d1244b3-5c9a-4f4c-a177-bc3f4e2fd587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829            NaN\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['embark_town'][825:830])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b0739a9-04da-4367-a4ab-5442a965272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southampton\n",
      "\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829    Southampton\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# embark_town 열의 NaN 값을 승선도시 중에서 가장 많이 출현한 값으로 치환하기\n",
    "most_freq = df['embark_town'].value_counts(dropna=True).idxmax() #NaN값을 제외하고 유요한 데이터 개수에서 최대 값을 추출\n",
    "print(most_freq)\n",
    "print('\\n')\n",
    "\n",
    "df['embark_town'].fillna(most_freq, inplace=True)\n",
    "print(df['embark_town'][825:830])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b29f78-f094-4bb4-a6d6-6e5969982b5b",
   "metadata": {},
   "source": [
    "NaN 값이 최빈값 Southampton로 치환이 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d4212-2336-4f8c-8b00-5b483c0cb341",
   "metadata": {},
   "source": [
    "실질적으로 데이터셋 중에는 누락 데이터가 NaN값으로 입력되지 않은 경우가 있다(숫자 0,'-','?')이것들과 같이 판다스에서 누락 데이터를 다루려면 replace() 메소드를 활용하여 Numpy에서 지원하는 np.nan 변경 해주는 것이 좋다.    \n",
    "df.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a272dab-e1ac-4409-9f78-ce41f0144d0e",
   "metadata": {},
   "source": [
    "데이터셋의 특성상 서로 이웃하고 있는 데이터끼리 유사성을 가질 가능성이 높다. 이럴 때는 앞이나 뒤에서 이웃라고 있는 값으로 치환해 주는 것이 좋다. fillna() 메소드에 method='ffill' ,method='bfill' 옵션을 적용하면 앞,뒤 값으로 치환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29621756-7f95-4d39-9dc2-87bd86a0755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829            NaN\n",
      "Name: embark_town, dtype: object\n",
      "\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829     Queenstown\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# titanic 데이터셋 가져오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# embark_town 열의 829행의 NaN 데이터 출력\n",
    "print(df['embark_town'][825:830])\n",
    "print('\\n')\n",
    "\n",
    "# embark_town 열의 NaN값을 바로 앞에 있는 828행의 값으로 변경하기\n",
    "df['embark_town'].fillna(method='ffill', inplace=True)\n",
    "print(df['embark_town'][825:830])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40586c0c-17b4-4ec7-aed6-d449643c90ee",
   "metadata": {},
   "source": [
    "## 중복 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd95a56-af06-4f9b-b15a-84820007c7b5",
   "metadata": {},
   "source": [
    "- 데이터프레임에서 각 행은 분석 대상이 갖고 있는 모든 속성에 대한 관측값을 뜻한다. 하나의 데이터셋에서 동일한 관측값이 2개 이상 중복되는 경우 중복 데이터를 찾아서 삭제해야한다.\n",
    "- 동일한 대상이 중복으로 존재하므로 분석 결과를 왜곡하기 떄문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2b660-4b55-4d15-800c-62ed836a90e8",
   "metadata": {},
   "source": [
    "### 중복 데이터 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918df5bd-8be0-4fa4-bb33-8438d62e4d48",
   "metadata": {},
   "source": [
    "- duplicated() 메소를 이용한다. 전에 나온 행들과 비교하여 중복되는 행이면 True를 반환하고 처음 나오는 행에 대해서는 False를 반환한다.\n",
    "- 각 행의 중복여부를 나타내는 불린 시리즈를 반환한다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb58e6f-d9d2-467b-b164-09754fab0c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: c2, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 중복 데이터를 갖는 데이터프레임 만들기\n",
    "df = pd.DataFrame({'c1':['a', 'a', 'b', 'a', 'b'],\n",
    "                  'c2':[1, 1, 1, 2, 2],\n",
    "                  'c3':[1, 1, 2, 2, 2]})\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "# 데이터프레임 전체 행 데이터 중에서 중복값 찾기\n",
    "df_dup = df.duplicated()\n",
    "print(df_dup)\n",
    "print('\\n')\n",
    "\n",
    "# 데이터프레임의 특정 열 데이터에서 중복값 찾기\n",
    "col_dup = df['c2'].duplicated()\n",
    "print(col_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15fd53-797d-46a2-a1f3-fef8e3ba48d3",
   "metadata": {},
   "source": [
    "### 중복 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634b863-693a-4eac-aa74-59023bc07eaa",
   "metadata": {},
   "source": [
    "- 중복 데이터를 제거하는 명령에는 drop_duplicates() 메소드를 사용한다.\n",
    "- 중복되는 행을 제거하고 고유한 관측값을 가진 행들만 남긴다.\n",
    "- 원본 객체를 변경하려면 inplace=True 옵션을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41645b9d-919d-4351-adde-b6dd6abee1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n",
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n",
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n"
     ]
    }
   ],
   "source": [
    "# 중복 데이터를 갖는 데이터프레임 만들기\n",
    "df = pd.DataFrame({'c1':['a', 'a', 'b', 'a', 'b'],\n",
    "                  'c2':[1, 1, 1, 2, 2],\n",
    "                  'c3':[1, 1, 2, 2, 2]})\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "# 데이터프레임에서 중복 행을 제거\n",
    "df2 = df.drop_duplicates()\n",
    "print(df2)\n",
    "print('\\n')\n",
    "\n",
    "# c2, c3열을 기준으로 중복 행을 제거\n",
    "df3 = df.drop_duplicates(subset=['c2', 'c3'])\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34336d-5b68-47a8-b6ed-ebd1a3336994",
   "metadata": {},
   "source": [
    "drop_duplicates() 메소드의 subset 옵션에 '열이름의 리스트'를 전달할 수 있다. 데이터의 중복 여부를 판별할 때 , subset옵션에 해당하는 열을 기준으로 판단하여 중복을 제거한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f95e33-1e04-487f-8ce7-e404ab62ff29",
   "metadata": {},
   "source": [
    "## 데이터 표준화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ac608-cbc9-4c9a-9620-ff3336270cde",
   "metadata": {},
   "source": [
    "- 여러 곳에서 수집한 자료들은 단위 선택, 대소문자 구분, 약칭 활용등 여러가지 원인에 의해 다양한 형태로 표현된다.\n",
    "- 서로 다른 단위가 섞여 있거나 같은 대상을 다른 형식으로 표현한 경우가 의외로 많다.\n",
    "- 이처럼 동일한 대상을 표현하는 방법에는 차이가 있으면 분석의 정확도는 현저히 낮아진다. 따라서 데이터 포멧을 일관성 있게 표준화하는 작업이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b8df0-2a44-46f7-97c7-0f4a16c066cb",
   "metadata": {},
   "source": [
    "### 단위 환산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf4052-cda3-4b84-883f-028af02cb967",
   "metadata": {},
   "source": [
    "같은 데이터 셋 안에서 서로 다른 측정 단위를 사용한다면, 전체 데이터의 일관성 측면에서 문제가 발생한다.   \n",
    "따라서 측정 단위를 동일하게 맞출 필요가 있다. 예제에서는 미국에서 쓰는 갤런당 마일단위로 연비를 표시하는 것을 리터당 킬로미터로 단위를 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9731c0c1-5c1a-4172-9d9b-877d69733aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0        130    3504          12.0          70   \n",
      "1  15.0          8         350.0        165    3693          11.5          70   \n",
      "2  18.0          8         318.0        150    3436          11.0          70   \n",
      "3  16.0          8         304.0        150    3433          12.0          70   \n",
      "4  17.0          8         302.0        140    3449          10.5          70   \n",
      "\n",
      "   origin                   car name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n",
      "3       1              amc rebel sst  \n",
      "4       1                ford torino  \n",
      "\n",
      "\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0        130    3504          12.0          70   \n",
      "1  15.0          8         350.0        165    3693          11.5          70   \n",
      "2  18.0          8         318.0        150    3436          11.0          70   \n",
      "\n",
      "   origin                   car name       kpl  \n",
      "0       1  chevrolet chevelle malibu  7.652571  \n",
      "1       1          buick skylark 320  6.377143  \n",
      "2       1         plymouth satellite  7.652571  \n",
      "\n",
      "\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0        130    3504          12.0          70   \n",
      "1  15.0          8         350.0        165    3693          11.5          70   \n",
      "2  18.0          8         318.0        150    3436          11.0          70   \n",
      "\n",
      "   origin                   car name   kpl  \n",
      "0       1  chevrolet chevelle malibu  7.65  \n",
      "1       1          buick skylark 320  6.38  \n",
      "2       1         plymouth satellite  7.65  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./auto-mpg.csv')\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "\n",
    "# mpg(mile per gallon)를 kpl(kilometer per liter)로 변환 (mpg_to_kpl = 0.425)\n",
    "mpg_to_kpl = 1.60934 / 3.78541\n",
    "\n",
    "# mpg 열에 0.425를 곱한 결과를 새로운 열(kpl)에 추가\n",
    "df['kpl'] = df['mpg'] * mpg_to_kpl\n",
    "print(df.head(3))    \n",
    "print('\\n')\n",
    "\n",
    "# kpl 열을 소수점 아래 둘째 자리에서 반올림 \n",
    "df['kpl'] = df['kpl'].round(2)\n",
    "print(df.head(3))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d63808-bbd5-4845-a91a-1640ab92ed9c",
   "metadata": {},
   "source": [
    "### 자료형 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df43fab-6db3-4a00-90c0-bee8cdb3013f",
   "metadata": {},
   "source": [
    "- 숫자가 문자열로 저장된 경우 숫자형으로 변환을 해줘야한다.\n",
    "- 먼저 dtypes 속성을 사용하여 데이터프레임을 구성하는 각 열의 자료형을 확인한다.(혹은 info() 메소드로 각 열의 자료형을 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2afbe18-8de8-4549-aa0b-7a8a96d1bdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg             float64\n",
      "cylinders         int64\n",
      "displacement    float64\n",
      "horsepower       object\n",
      "weight            int64\n",
      "acceleration    float64\n",
      "model year        int64\n",
      "origin            int64\n",
      "car name         object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "['130' '165' '150' '140' '198' '220' '215' '225' '190' '170' '160' '95'\n",
      " '97' '85' '88' '46' '87' '90' '113' '200' '210' '193' '?' '100' '105'\n",
      " '175' '153' '180' '110' '72' '86' '70' '76' '65' '69' '60' '80' '54'\n",
      " '208' '155' '112' '92' '145' '137' '158' '167' '94' '107' '230' '49' '75'\n",
      " '91' '122' '67' '83' '78' '52' '61' '93' '148' '129' '96' '71' '98' '115'\n",
      " '53' '81' '79' '120' '152' '102' '108' '68' '58' '149' '89' '63' '48'\n",
      " '66' '139' '103' '125' '133' '138' '135' '142' '77' '62' '132' '84' '64'\n",
      " '74' '116' '82']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./auto-mpg.csv')\n",
    "# 각 열의 자료형 확인\n",
    "print(df.dtypes)\n",
    "print('\\n')\n",
    "\n",
    "# 문자열을 뜻하는 object 자료형인 'horsepower' 열의 고유값 확인\n",
    "print(df['horsepower'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cac165-7ed4-4b7e-8d99-c6ef838e874c",
   "metadata": {},
   "source": [
    "엔진 출력의 크기를 나타내는 데이터 만큼 숫자형으로 변환해주는 것이 적절하다. 하지만 문자열로 저장된 이유는 고유값에서 ?가 있기 때문이다.    \n",
    "?가 포함된 데이터를 NaN 값으로 변환을 하고 dropna() 메소드로 NaN값을 포함하는 모든 행을 삭제를 할 것이다. 그리고 데이터타입을 변환하는 astype() 메소드를 활용하여 실수형 데이터 타입으로 변환하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293732a9-29a1-4e51-89b1-2ab3220a87ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[130. 165. 150. 140. 198. 220. 215. 225. 190. 170. 160.  95.  97.  85.\n",
      "  88.  46.  87.  90. 113. 200. 210. 193. 100. 105. 175. 153. 180. 110.\n",
      "  72.  86.  70.  76.  65.  69.  60.  80.  54. 208. 155. 112.  92. 145.\n",
      " 137. 158. 167.  94. 107. 230.  49.  75.  91. 122.  67.  83.  78.  52.\n",
      "  61.  93. 148. 129.  96.  71.  98. 115.  53.  81.  79. 120. 152. 102.\n",
      " 108.  68.  58. 149.  89.  63.  48.  66. 139. 103. 125. 133. 138. 135.\n",
      " 142.  77.  62. 132.  84.  64.  74. 116.  82.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df['horsepower'].replace('?', np.nan, inplace=True) #?을 NaN 값으로 대치\n",
    "df.dropna(subset=['horsepower'], axis=0, inplace=True) #horsepower 열 nan값 포함되는 값 행 제거\n",
    "df['horsepower'] = df['horsepower'].astype('float') # 형변환\n",
    "\n",
    "print(df['horsepower'].dtypes)\n",
    "print(df['horsepower'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4925c4-ebde-411d-bbc2-e3aa73044e88",
   "metadata": {},
   "source": [
    "또한 'origin' 열에서 정수형 데이터 1,2,3이 들어가 있지만 실제로는 국가 이름인'usa, eu, jpn' 을 뜻하기 때문에 각 숫자 데이터를 국가 이름으로 바꿔 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "889b1391-9cbf-4d73-987d-404dd0044c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USA' 'JAPAN' 'EU']\n",
      "['USA' 'JAPAN' 'EU']\n",
      "object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# origin 열의 고유값 확인\n",
    "print(df['origin'].unique())\n",
    "\n",
    "# 정수형 데이터를 문자형 데이터로 변환 \n",
    "df['origin'].replace({1:'USA', 2:'EU', 3:'JAPAN'}, inplace=True)\n",
    "\n",
    "# origin 열의 고유값과 자료형 확인\n",
    "print(df['origin'].unique())\n",
    "print(df['origin'].dtypes) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b51961-9c23-498b-ae1f-35050d9c9fe2",
   "metadata": {},
   "source": [
    "3개 국가 이름이 반복된다. 이처럼 유한 개의 고유값이 반복적으로 나타나는 경우에는 범주형(category) 데이터로 표현하는 것이 효율적이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73e7f7b0-6e36-41cf-9e48-13f849b66caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# origin 열의 문자열 자료형을 범주형으로 변환\n",
    "df['origin'] = df['origin'].astype('category')     \n",
    "print(df['origin'].dtypes) \n",
    "\n",
    "# 범주형을 문자열로 다시 변환\n",
    "df['origin'] = df['origin'].astype('str')     \n",
    "print(df['origin'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f123fed-78fb-4cd6-8d2c-6d325ea58f3e",
   "metadata": {},
   "source": [
    "마지막으로 'model_year' 열의 자료형을 살펴보면 모델 출시연도가 숫자로 기록이 되어 있고 , 자료형은 정수형을 나타낸다. 연도를 뜻하기 때문에 숫자형으로 남겨둬도 무방하지만 연도는 시간적인 순서의 의미는 있으나 숫자의 상대적인 크기는 별 의미가 없다. 따라서 데이터는 숫자형태를 갖더라도 자료형은 범주형으로 표현하는 것이 적절하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84918ef1-6ac8-44c4-a59f-f64646f113a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63     72\n",
      "381    82\n",
      "378    82\n",
      "Name: model year, dtype: int64\n",
      "125    74\n",
      "129    74\n",
      "120    73\n",
      "Name: model year, dtype: category\n",
      "Categories (13, int64): [70, 71, 72, 73, ..., 79, 80, 81, 82]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model year 열의 정수형을 범주형으로 변환\n",
    "print(df['model year'].sample(3))\n",
    "df['model year'] = df['model year'].astype('category') \n",
    "print(df['model year'].sample(3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae91c0-77da-4bd8-996e-6893a6aab743",
   "metadata": {},
   "source": [
    "## 범주형(카테고리) 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306d99f-ff00-417a-a15e-d3eed5918312",
   "metadata": {},
   "source": [
    "### 구간 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56425468-ea0c-447f-a8be-f4398dc59b16",
   "metadata": {},
   "source": [
    "- 데이터 분석 알고리즘에 따라서는 연속 데이터를 그래도 사용하기 보다는 일정한 구간으로 나눠서 분석하는 것이 효율적인 경우가 있다.\n",
    "- 가격,비용, 효율 등 연속적인 값을 일정한 수준이나 정도로 나타내는 이산적(범주형)으로 값으로 나타내어 구간별 차이를 드러내는 것이다. \n",
    "- 연속 변수를 일정한 구간으로 나누고, 각 구간을 범주형 이산 변수로 변환하는 과정을 구간 분할이라고 한다.\n",
    "- 판다스에서 cut() 메소드를 이용하면 연속데이터를 여러구간으로 나누고 범주형 데이터로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02535bc-5101-45d1-b47a-72ecbbb676e5",
   "metadata": {},
   "source": [
    "auto-mpg 데이터셋에서 'horsepower' 열을 구간으로 나누어 '저출력', '보통출력', '고출력' 등 구간으로 나누어서 표시해보자. 3개 구간을 구분하려면 4개의 경계값이 필요하다. 이는 numpy 라이브러리에서 histogram() 함수를 활용한 방법으로 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6912d159-3fcf-4018-8370-4549d53205c0",
   "metadata": {},
   "source": [
    "histogram() 함수는 나누려는 구간(bin)개수를 bins옵션에 입력하면 각 구간에 속하는 값에 속하는 값의 개수(count), 경계값 리스트(bin_dividers)를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0661457a-4fbf-419a-9c40-0382ff86c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46.         107.33333333 168.66666667 230.        ]\n",
      "    horsepower hp_bin\n",
      "0        130.0   보통출력\n",
      "1        165.0   보통출력\n",
      "2        150.0   보통출력\n",
      "3        150.0   보통출력\n",
      "4        140.0   보통출력\n",
      "5        198.0    고출력\n",
      "6        220.0    고출력\n",
      "7        215.0    고출력\n",
      "8        225.0    고출력\n",
      "9        190.0    고출력\n",
      "10       170.0    고출력\n",
      "11       160.0   보통출력\n",
      "12       150.0   보통출력\n",
      "13       225.0    고출력\n",
      "14        95.0    저출력\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read_csv() 함수로 df 생성\n",
    "df = pd.read_csv('./auto-mpg.csv')\n",
    "\n",
    "# horsepower 열의 누락 데이터('?') 삭제하고 실수형으로 변환\n",
    "df['horsepower'].replace('?', np.nan, inplace=True)      # '?'을 np.nan으로 변경\n",
    "df.dropna(subset=['horsepower'], axis=0, inplace=True)   # 누락데이터 행을 삭제\n",
    "df['horsepower'] = df['horsepower'].astype('float')      # 문자열을 실수형으로 변환\n",
    "\n",
    "# np.histogram 함수로 3개의 bin으로 나누는 경계 값의 리스트 구하기\n",
    "count, bin_dividers = np.histogram(df['horsepower'], bins=3)\n",
    "print(bin_dividers) \n",
    "\n",
    "# 3개의 bin에 이름 지정\n",
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "\n",
    "# pd.cut 함수로 각 데이터를 3개의 bin에 할당\n",
    "df['hp_bin'] = pd.cut(x=df['horsepower'],     # 데이터 배열\n",
    "                      bins=bin_dividers,      # 경계 값 리스트\n",
    "                      labels=bin_names,       # bin 이름\n",
    "                      include_lowest=True)    # 첫 경계값 포함 \n",
    "\n",
    "# horsepower 열, hp_bin 열의 첫 15행을 출력\n",
    "print(df[['horsepower', 'hp_bin']].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f54c4-7d3e-4e51-b7da-d39151c07736",
   "metadata": {},
   "source": [
    "### 더미 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ad6027-ad0e-44ec-a19d-792c358c353b",
   "metadata": {},
   "source": [
    "- 앞에서 'horsepower' 열을 범주형 데이터로 변환을 했는대, 이처럼 카테고리를 나타내는 범주형 데이터를 회귀분석등 머신러닝 알고리즘에 반로 사용할 수 없다. 왜냐면 범주형 데이터는 컴퓨터가 인식 가능한 값이 아니기 떄문이다.\n",
    "- 이때 필요한 변환 작업이 0또는 1로 표현되는 더비 변수를 사용하는 것이다. 여기서 0,1는 수치의 크기에 의미 보다는 어떤 특성이 있는지 없는지 여부이다.\n",
    "- 이를 다른 말로 one-hot-encoding이라고도 부른다.\n",
    "- get_dummies() 함수를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eed28b1a-159a-44d2-88de-a81aceb24b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저출력  보통출력  고출력\n",
      "0     0     1    0\n",
      "1     0     1    0\n",
      "2     0     1    0\n",
      "3     0     1    0\n",
      "4     0     1    0\n",
      "5     0     0    1\n",
      "6     0     0    1\n",
      "7     0     0    1\n",
      "8     0     0    1\n",
      "9     0     0    1\n",
      "10    0     0    1\n",
      "11    0     1    0\n",
      "12    0     1    0\n",
      "13    0     0    1\n",
      "14    1     0    0\n"
     ]
    }
   ],
   "source": [
    "# hp_bin 열의 범주형 데이터를 더미 변수로 변환\n",
    "horsepower_dummies = pd.get_dummies(df['hp_bin'])\n",
    "print(horsepower_dummies.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f5294-951d-4994-9a97-3cf12bbf4410",
   "metadata": {},
   "source": [
    "또한 sklearn 라이브러리를 이용해서 원핫인코딩을 편하게 처리가 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1937bc2-2ef0-4c56-975d-a45d000adf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 0 1 1 0 2]\n",
      "<class 'numpy.ndarray'>\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]]\n",
      "<class 'numpy.ndarray'>\n",
      "  (0, 1)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 1)\t1.0\n",
      "  (12, 1)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 2)\t1.0\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# sklern 라이브러리 불러오기\n",
    "from sklearn import preprocessing    \n",
    "\n",
    "# 전처리를 위한 encoder 객체 만들기\n",
    "label_encoder = preprocessing.LabelEncoder()       # label encoder 생성\n",
    "onehot_encoder = preprocessing.OneHotEncoder()   # one hot encoder 생성\n",
    "\n",
    "# label encoder로 문자열 범주를 숫자형 범주로 변환\n",
    "onehot_labeled = label_encoder.fit_transform(df['hp_bin'].head(15))  \n",
    "print(onehot_labeled)\n",
    "print(type(onehot_labeled))\n",
    "\n",
    "# 2차원 행렬로 형태 변경\n",
    "onehot_reshaped = onehot_labeled.reshape(len(onehot_labeled), 1) \n",
    "print(onehot_reshaped)\n",
    "print(type(onehot_reshaped))\n",
    "\n",
    "# 희소행렬로 변환\n",
    "onehot_fitted = onehot_encoder.fit_transform(onehot_reshaped)\n",
    "print(onehot_fitted)\n",
    "print(type(onehot_fitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48c22d-9c08-4b72-bbb2-bbf0303fbd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
